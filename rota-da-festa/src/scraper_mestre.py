import os
import asyncio
import re
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from supabase import create_client, Client
from dotenv import load_dotenv
from geopy.geocoders import Nominatim
from playwright.async_api import async_playwright

# Carregar envs
load_dotenv()
load_dotenv("../.env.local")
load_dotenv("../../rota-da-festa-web/.env.local")

# Supabase Setup
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY") or os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("‚ùå Credenciais Supabase em falta.")
    exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
geolocator = Nominatim(user_agent="rota_da_festa_bot_v5")

# ========================================================================
# Cache de est√°dios ‚Äî profissionais + semi-profissionais + distritais
# ========================================================================
CACHE_ESTADIOS = {
    # --- Liga Portugal (Primeira Liga) ---
    "Benfica": {"lat": 38.7527, "lon": -9.1847, "local": "Est√°dio da Luz"},
    "Sporting": {"lat": 38.7614, "lon": -9.1608, "local": "Est√°dio Jos√© Alvalade"},
    "FC Porto": {"lat": 41.1617, "lon": -8.5839, "local": "Est√°dio do Drag√£o"},
    "SC Braga": {"lat": 41.5617, "lon": -8.4309, "local": "Est√°dio Municipal de Braga"},
    "Braga": {"lat": 41.5617, "lon": -8.4309, "local": "Est√°dio Municipal de Braga"},
    "Vit√≥ria SC": {"lat": 41.4468, "lon": -8.2974, "local": "Est√°dio D. Afonso Henriques"},
    "Vit√≥ria": {"lat": 41.4468, "lon": -8.2974, "local": "Est√°dio D. Afonso Henriques"},
    "Moreirense": {"lat": 41.3831, "lon": -8.3364, "local": "Parque Comendador Joaquim de Almeida Freitas"},
    "Famalic√£o": {"lat": 41.4111, "lon": -8.5273, "local": "Est√°dio Municipal de Famalic√£o"},
    "Gil Vicente": {"lat": 41.5372, "lon": -8.6339, "local": "Est√°dio Cidade de Barcelos"},
    "Rio Ave": {"lat": 41.3638, "lon": -8.7401, "local": "Est√°dio dos Arcos"},
    "Arouca": {"lat": 40.9333, "lon": -8.2439, "local": "Est√°dio Municipal de Arouca"},
    "Boavista": {"lat": 41.1614, "lon": -8.6425, "local": "Est√°dio do Bessa"},
    "Casa Pia": {"lat": 38.7539, "lon": -9.2342, "local": "Est√°dio Pina Manique"},
    "Estoril": {"lat": 38.7067, "lon": -9.3978, "local": "Est√°dio Ant√≥nio Coimbra da Mota"},
    "Estrela Amadora": {"lat": 38.7539, "lon": -9.2342, "local": "Est√°dio Jos√© Gomes"},
    "Est. Amadora": {"lat": 38.7539, "lon": -9.2342, "local": "Est√°dio Jos√© Gomes"},
    "Santa Clara": {"lat": 37.7500, "lon": -25.6667, "local": "Est√°dio de S√£o Miguel"},
    "Nacional": {"lat": 32.6476, "lon": -16.9316, "local": "Est√°dio da Madeira"},
    "AVS": {"lat": 41.3638, "lon": -8.7401, "local": "Est√°dio dos Arcos"},
    "Farense": {"lat": 37.0156, "lon": -7.9275, "local": "Est√°dio de S√£o Lu√≠s"},
    # --- Liga Portugal 2 ---
    "Leix√µes": {"lat": 41.1833, "lon": -8.7000, "local": "Est√°dio do Mar"},
    "Vizela": {"lat": 41.3789, "lon": -8.3075, "local": "Est√°dio do FC Vizela"},
    "FC Vizela": {"lat": 41.3789, "lon": -8.3075, "local": "Est√°dio do FC Vizela"},
    "Tondela": {"lat": 40.5167, "lon": -8.0833, "local": "Est√°dio Jo√£o Cardoso"},
    "CD Tondela": {"lat": 40.5167, "lon": -8.0833, "local": "Est√°dio Jo√£o Cardoso"},
    "Acad√©mica": {"lat": 40.2109, "lon": -8.4377, "local": "Est√°dio Cidade de Coimbra"},
    "Penafiel": {"lat": 41.2083, "lon": -8.2833, "local": "Est√°dio Municipal 25 de Abril"},
    "Feirense": {"lat": 40.9255, "lon": -8.5414, "local": "Est√°dio Marcolino de Castro"},
    "Oliveirense": {"lat": 40.8386, "lon": -8.4776, "local": "Est√°dio Carlos Os√≥rio"},
    "Chaves": {"lat": 41.7431, "lon": -7.4714, "local": "Est√°dio Municipal Eng. Manuel Branco Teixeira"},
    "Desportivo de Chaves": {"lat": 41.7431, "lon": -7.4714, "local": "Est√°dio Municipal Eng. Manuel Branco Teixeira"},
    "Portimonense": {"lat": 37.1326, "lon": -8.5379, "local": "Est√°dio Municipal de Portim√£o"},
    "Mar√≠timo": {"lat": 32.6476, "lon": -16.9316, "local": "Est√°dio dos Barreiros"},
    "Pa√ßos Ferreira": {"lat": 41.2764, "lon": -8.3886, "local": "Est√°dio Capital do M√≥vel"},
    "Pa√ßos de Ferreira": {"lat": 41.2764, "lon": -8.3886, "local": "Est√°dio Capital do M√≥vel"},
    "Varzim": {"lat": 41.3833, "lon": -8.7667, "local": "Est√°dio do Varzim SC"},
    "Trofense": {"lat": 41.3333, "lon": -8.5500, "local": "Est√°dio do CD Trofense"},
    "Mafra": {"lat": 38.9400, "lon": -9.3275, "local": "Est√°dio Municipal Dr. M√°rio Silveira"},
    "Alverca": {"lat": 38.8953, "lon": -9.0392, "local": "Est√°dio do FC Alverca"},
    "Benfica B": {"lat": 38.7527, "lon": -9.1847, "local": "Caixa Futebol Campus"},
    "Porto B": {"lat": 41.1617, "lon": -8.5839, "local": "Est√°dio do Olival"},
    # --- Liga 3 / Campeonato de Portugal ---
    "Beira-Mar": {"lat": 40.6416, "lon": -8.6064, "local": "Est√°dio Municipal de Aveiro"},
    "Sp. Covilh√£": {"lat": 40.2833, "lon": -7.5000, "local": "Est√°dio Santos Pinto"},
    "Acad√©mico Viseu": {"lat": 40.6610, "lon": -7.9097, "local": "Est√°dio do Fontelo"},
    "Real SC": {"lat": 38.7539, "lon": -9.2342, "local": "Est√°dio Municipal de Rio Maior"},
    "Belenenses": {"lat": 38.7025, "lon": -9.2067, "local": "Est√°dio do Restelo"},
    "Cova da Piedade": {"lat": 38.6667, "lon": -9.1500, "local": "Est√°dio Municipal de Almada"},
    "Felgueiras": {"lat": 41.3642, "lon": -8.1978, "local": "Est√°dio Municipal de Felgueiras"},
    "Fafe": {"lat": 41.4500, "lon": -8.1667, "local": "Parque Municipal de Desportos de Fafe"},
    "AD Fafe": {"lat": 41.4500, "lon": -8.1667, "local": "Parque Municipal de Desportos de Fafe"},
    "Amarante": {"lat": 41.2717, "lon": -8.0750, "local": "Est√°dio Municipal de Amarante"},
    "Tirsense": {"lat": 41.3444, "lon": -8.4758, "local": "Est√°dio Municipal de Santo Tirso"},
    "Gondomar": {"lat": 41.1444, "lon": -8.5333, "local": "Est√°dio de S√£o Miguel"},
    "Espinho": {"lat": 41.0068, "lon": -8.6291, "local": "Est√°dio Comendador Manuel Violas"},
    "SC Espinho": {"lat": 41.0068, "lon": -8.6291, "local": "Est√°dio Comendador Manuel Violas"},
    "Le√ßa": {"lat": 41.1833, "lon": -8.7000, "local": "Est√°dio do Le√ßa FC"},
    "Le√ßa FC": {"lat": 41.1833, "lon": -8.7000, "local": "Est√°dio do Le√ßa FC"},
    "Maia": {"lat": 41.2333, "lon": -8.6167, "local": "Est√°dio Prof. Dr. Jos√© Vieira de Carvalho"},
    "Limianos": {"lat": 41.7667, "lon": -8.5833, "local": "Est√°dio Municipal de Ponte de Lima"},
    # --- AF BRAGA (distritais) ---
    "Merelinense": {"lat": 41.5768, "lon": -8.4482, "local": "Est√°dio Jo√£o Soares Vieira"},
    "Merelinense FC": {"lat": 41.5768, "lon": -8.4482, "local": "Est√°dio Jo√£o Soares Vieira"},
    "Vilaverdense": {"lat": 41.6489, "lon": -8.4356, "local": "Campo Cruz do Reguengo"},
    "Vilaverdense FC": {"lat": 41.6489, "lon": -8.4356, "local": "Campo Cruz do Reguengo"},
    "Maria da Fonte": {"lat": 41.6032, "lon": -8.2589, "local": "Est√°dio Moinhos Novos"},
    "Dumiense": {"lat": 41.5621, "lon": -8.4328, "local": "Campo Celestino Lobo"},
    "Dumiense FC": {"lat": 41.5621, "lon": -8.4328, "local": "Campo Celestino Lobo"},
    "GD Joane": {"lat": 41.4333, "lon": -8.4167, "local": "Est√°dio de Barreiros, Joane"},
    "Brito SC": {"lat": 41.4886, "lon": -8.3582, "local": "Parque de Jogos do Brito SC"},
    "Brito": {"lat": 41.4886, "lon": -8.3582, "local": "Parque de Jogos do Brito SC"},
    "Santa Maria FC": {"lat": 41.5333, "lon": -8.5333, "local": "Est√°dio da Devesa"},
    "Vieira SC": {"lat": 41.6333, "lon": -8.1333, "local": "Est√°dio Municipal de Vieira"},
    "AD Ninense": {"lat": 41.4667, "lon": -8.5500, "local": "Complexo Desportivo de Nine"},
    "GD Prado": {"lat": 41.6000, "lon": -8.4667, "local": "Complexo Desportivo do Faial"},
    "Pevid√©m SC": {"lat": 41.4167, "lon": -8.3333, "local": "Parque Albano Martins Coelho Lima"},
    "Pevid√©m": {"lat": 41.4167, "lon": -8.3333, "local": "Parque Albano Martins Coelho Lima"},
    "Ca√ßadores Taipas": {"lat": 41.4833, "lon": -8.3500, "local": "Est√°dio do Montinho"},
    "Ca√ßadores das Taipas": {"lat": 41.4833, "lon": -8.3500, "local": "Est√°dio do Montinho"},
    "Ber√ßo SC": {"lat": 41.4667, "lon": -8.3333, "local": "Complexo Desportivo de Ponte"},
    "CD Celeir√≥s": {"lat": 41.5167, "lon": -8.4500, "local": "Parque Desportivo de Celeir√≥s"},
    "Forj√£es SC": {"lat": 41.6167, "lon": -8.7333, "local": "Est√°dio Hor√°cio Queir√≥s"},
    "Desportivo de Ronfe": {"lat": 41.4333, "lon": -8.3667, "local": "Est√°dio do Desportivo de Ronfe"},
    "Sandineneses": {"lat": 41.4667, "lon": -8.3833, "local": "Complexo Desportivo D. Maria Teresa"},
    "Acad√©mico": {"lat": 41.5503, "lon": -8.4270, "local": "Est√°dio 1¬∫ de Maio, Braga"},
    "Sanjoanense": {"lat": 41.0333, "lon": -8.5000, "local": "Est√°dio da Sanjoanense"},
    "Caldelas SC": {"lat": 41.5833, "lon": -8.2667, "local": "Campo Municipal de Caldelas"},
    "Taipas": {"lat": 41.4833, "lon": -8.3500, "local": "Est√°dio do Montinho"},
    "FC Penafiel": {"lat": 41.2083, "lon": -8.2833, "local": "Est√°dio Municipal 25 de Abril"},
    "Ribeir√£o": {"lat": 41.5000, "lon": -8.4667, "local": "Campo de Ribeir√£o"},
    "Ar√µes SC": {"lat": 41.5333, "lon": -8.2667, "local": "Campo de Ar√µes"},
    "Ar√µes": {"lat": 41.5333, "lon": -8.2667, "local": "Campo de Ar√µes"},
    "Serzedelo": {"lat": 41.4333, "lon": -8.3333, "local": "Campo de Serzedelo"},
    "Martim": {"lat": 41.5333, "lon": -8.3500, "local": "Campo de Martim"},
    "Cabreiros": {"lat": 41.5833, "lon": -8.4333, "local": "Campo de Cabreiros"},
    "Esposende": {"lat": 41.5333, "lon": -8.7833, "local": "Est√°dio Municipal de Esposende"},
    "SC Esposende": {"lat": 41.5333, "lon": -8.7833, "local": "Est√°dio Municipal de Esposende"},
    "Bairro": {"lat": 41.5000, "lon": -8.3500, "local": "Campo do Bairro"},
    "Santa Eul√°lia": {"lat": 41.5833, "lon": -8.3500, "local": "Campo de Santa Eul√°lia"},
    "Porto d'Ave": {"lat": 41.5167, "lon": -8.3167, "local": "Campo de Porto d'Ave"},
    "Ruilhe": {"lat": 41.5667, "lon": -8.4833, "local": "Campo de Ruilhe"},
    "Pedralva": {"lat": 41.5500, "lon": -8.4000, "local": "Campo de Pedralva"},
    "Maximinos": {"lat": 41.5667, "lon": -8.4333, "local": "Campo de Maximinos"},
    "Real SC Braga": {"lat": 41.5503, "lon": -8.4270, "local": "Est√°dio 1¬∫ de Maio, Braga"},
    "Palmeiras FC": {"lat": 41.5500, "lon": -8.4667, "local": "Campo de Palmeiras, Braga"},
    "Nogueir√≥ e Ten√µes": {"lat": 41.5350, "lon": -8.4050, "local": "Campo de Nogueir√≥"},
    "SC Barcelos": {"lat": 41.5372, "lon": -8.6339, "local": "Est√°dio Cidade de Barcelos"},
    # --- AF VIANA DO CASTELO (distritais) ---
    "Mon√ß√£o": {"lat": 42.0769, "lon": -8.4822, "local": "Est√°dio Municipal de Mon√ß√£o"},
    "Cerveira": {"lat": 41.9406, "lon": -8.7442, "local": "Campo Municipal de Vila Nova de Cerveira"},
    "Limianos": {"lat": 41.7667, "lon": -8.6333, "local": "Campo dos Limianos, Ponte de Lima"},
    # --- AF AVEIRO (distritais) ---
    "ADC Lob√£o": {"lat": 40.9634, "lon": -8.4876, "local": "Parque de Jogos de Lob√£o"},
    "Fi√£es SC": {"lat": 40.9921, "lon": -8.5235, "local": "Est√°dio do Bolh√£o"},
    "Fi√£es": {"lat": 40.9921, "lon": -8.5235, "local": "Est√°dio do Bolh√£o"},
    "RD √Ågueda": {"lat": 40.5744, "lon": -8.4485, "local": "Est√°dio Municipal de √Ågueda"},
    "√Ågueda": {"lat": 40.5744, "lon": -8.4485, "local": "Est√°dio Municipal de √Ågueda"},
    "Ovarense": {"lat": 40.8594, "lon": -8.6269, "local": "Est√°dio Municipal de Ovar"},
    "Pampilhosa": {"lat": 40.4500, "lon": -8.3833, "local": "Campo da Pampilhosa"},
    "Estarreja": {"lat": 40.7558, "lon": -8.5711, "local": "Est√°dio Municipal de Estarreja"},
    "Anadia": {"lat": 40.4357, "lon": -8.4357, "local": "Est√°dio Municipal de Anadia"},
    "Lusitano de √âvora": {"lat": 38.5667, "lon": -7.9000, "local": "Est√°dio do Lusitano de √âvora"},
    "Vit√≥ria Set√∫bal": {"lat": 38.5244, "lon": -8.8882, "local": "Est√°dio do Bonfim"},
    "Leiria": {"lat": 39.7500, "lon": -8.8000, "local": "Est√°dio Dr. Magalh√£es Pessoa"},
    "Covilh√£": {"lat": 40.2833, "lon": -7.5000, "local": "Est√°dio Santos Pinto"},
    # --- AF PORTO (distritais) ---
    "SC Rio Tinto": {"lat": 41.1764, "lon": -8.5583, "local": "Est√°dio Cidade de Rio Tinto"},
    "Rio Tinto": {"lat": 41.1764, "lon": -8.5583, "local": "Est√°dio Cidade de Rio Tinto"},
    "Gondomar SC": {"lat": 41.1444, "lon": -8.5333, "local": "Est√°dio de S√£o Miguel, Gondomar"},
    "Maia Lidador": {"lat": 41.2333, "lon": -8.6167, "local": "Est√°dio Prof. Dr. Jos√© Vieira de Carvalho"},
    "Pedras Rubras": {"lat": 41.2333, "lon": -8.6833, "local": "Campo de Pedras Rubras"},
    "Drag√µes Sandinenses": {"lat": 41.3167, "lon": -8.5500, "local": "Campo dos Drag√µes Sandinenses"},
    "Vila Me√£": {"lat": 41.1500, "lon": -8.3833, "local": "Campo de Vila Me√£"},
    "Foz": {"lat": 41.1500, "lon": -8.6833, "local": "Campo da Foz do Douro"},
    "Canidelo": {"lat": 41.1333, "lon": -8.6500, "local": "Campo de Canidelo"},
    "Infesta": {"lat": 41.2000, "lon": -8.6000, "local": "Campo de Infesta"},
    "Padroense": {"lat": 41.1833, "lon": -8.6167, "local": "Campo do Padroense FC"},
    "Salgueiros": {"lat": 41.1500, "lon": -8.6333, "local": "Est√°dio Engenheiro Vidal Pinheiro"},
    "Campanh√£": {"lat": 41.1500, "lon": -8.5833, "local": "Campo de Campanh√£"},
    "√Åguias Moreira": {"lat": 41.1833, "lon": -8.7333, "local": "Campo das √Åguias de Moreira"},
    "Nogueirense": {"lat": 41.2333, "lon": -8.5500, "local": "Campo do Nogueirense"},
    "Lusit√¢nia Lourosa": {"lat": 40.9833, "lon": -8.5333, "local": "Est√°dio do Lusit√¢nia de Lourosa"},
    "Lourosa": {"lat": 40.9833, "lon": -8.5333, "local": "Est√°dio do Lusit√¢nia de Lourosa"},
}

# Centr√≥ides dos distritos/AFs ‚Äî fallback quando geocoding falha
DISTRICT_CENTROIDS = {
    "braga": {"lat": 41.5503, "lon": -8.4270, "local": "Braga (aproximado)"},
    "porto": {"lat": 41.1496, "lon": -8.6109, "local": "Porto (aproximado)"},
    "aveiro": {"lat": 40.6405, "lon": -8.6538, "local": "Aveiro (aproximado)"},
    "lisboa": {"lat": 38.7223, "lon": -9.1393, "local": "Lisboa (aproximado)"},
    "leiria": {"lat": 39.7437, "lon": -8.8070, "local": "Leiria (aproximado)"},
    "coimbra": {"lat": 40.2109, "lon": -8.4377, "local": "Coimbra (aproximado)"},
    "viseu": {"lat": 40.6610, "lon": -7.9097, "local": "Viseu (aproximado)"},
    "set√∫bal": {"lat": 38.5244, "lon": -8.8882, "local": "Set√∫bal (aproximado)"},
    "santar√©m": {"lat": 39.2369, "lon": -8.6850, "local": "Santar√©m (aproximado)"},
    "beja": {"lat": 38.0150, "lon": -7.8653, "local": "Beja (aproximado)"},
    "faro": {"lat": 37.0194, "lon": -7.9304, "local": "Faro (aproximado)"},
    "√©vora": {"lat": 38.5667, "lon": -7.9000, "local": "√âvora (aproximado)"},
    "bragan√ßa": {"lat": 41.8063, "lon": -6.7572, "local": "Bragan√ßa (aproximado)"},
    "castelo branco": {"lat": 39.8228, "lon": -7.4906, "local": "Castelo Branco (aproximado)"},
    "guarda": {"lat": 40.5373, "lon": -7.2676, "local": "Guarda (aproximado)"},
    "viana": {"lat": 41.6936, "lon": -8.8319, "local": "Viana do Castelo (aproximado)"},
    "viana do castelo": {"lat": 41.6936, "lon": -8.8319, "local": "Viana do Castelo (aproximado)"},
    "vila real": {"lat": 41.2959, "lon": -7.7464, "local": "Vila Real (aproximado)"},
    "portalegre": {"lat": 39.2967, "lon": -7.4317, "local": "Portalegre (aproximado)"},
    "funchal": {"lat": 32.6669, "lon": -16.9241, "local": "Funchal (aproximado)"},
    "ponta delgada": {"lat": 37.7483, "lon": -25.6666, "local": "Ponta Delgada (aproximado)"},
    "angra do hero√≠smo": {"lat": 38.6545, "lon": -27.2177, "local": "Angra do Hero√≠smo (aproximado)"},
    "horta": {"lat": 38.5342, "lon": -28.6300, "local": "Horta (aproximado)"},
}

# Palavras-chave de competi√ß√µes portuguesas
PORTUGUESE_COMP_KEYWORDS = [
    "portugal",
    "liga portugal", "primeira liga", "liga 2", "segunda liga", "meu super",
    "liga 3", "campeonato de portugal", "ta√ßa de portugal", "ta√ßa da liga",
    "superta√ßa", "liga revela√ß√£o", "ta√ßa revela√ß√£o",
    "af braga", "af porto", "af aveiro", "af lisboa", "af leiria",
    "af coimbra", "af viseu", "af set√∫bal", "af santar√©m", "af beja",
    "af faro", "af √©vora", "af bragan√ßa", "af castelo branco",
    "af guarda", "af viana", "af vila real", "af portalegre",
    "af funchal", "af ponta delgada", "af angra", "af horta",
    "pro-nacional", "pr√≥-nacional", "distrital", "divis√£o de honra",
    "1¬™ divis√£o", "2¬™ divis√£o", "3¬™ divis√£o", "divis√£o elite",
    "liga regional", "campeonato regional",
]

# Nomes de equipas portuguesas (para detectar em competi√ß√µes internacionais)
PORTUGUESE_TEAMS = list(CACHE_ESTADIOS.keys())


def _team_match(pt_name: str, team_name: str) -> bool:
    """Verifica se o nome da equipa portuguesa corresponde ao nome dado."""
    pl = pt_name.lower()
    tl = team_name.lower().strip()
    if tl == pl:
        return True
    return pl in tl and len(pl) / len(tl) > 0.55


def _extract_district(comp_text: str):
    """Extrai o distrito/AF do texto da competi√ß√£o e devolve o centr√≥ide."""
    cl = comp_text.lower()
    for district, geo in DISTRICT_CENTROIDS.items():
        if f"af {district}" in cl:
            return geo
    return None


def geolocalizar_estadio(nome_equipa: str, comp_text: str = ""):
    """Localiza o est√°dio de uma equipa com m√∫ltiplos fallbacks."""
    # 1. Cache
    for k, v in CACHE_ESTADIOS.items():
        if _team_match(k, nome_equipa):
            return v

    # 2. Nominatim: "Est√°dio {equipa}, Portugal"
    try:
        loc = geolocator.geocode(f"Est√°dio {nome_equipa}, Portugal", timeout=5)
        if loc:
            result = {"lat": loc.latitude, "lon": loc.longitude, "local": loc.address.split(",")[0]}
            CACHE_ESTADIOS[nome_equipa] = result
            return result
    except Exception:
        pass
    time.sleep(1.1)

    # 3. Nominatim: "{equipa} futebol, Portugal"
    try:
        loc = geolocator.geocode(f"{nome_equipa} futebol, Portugal", timeout=5)
        if loc:
            result = {"lat": loc.latitude, "lon": loc.longitude, "local": f"Campo {nome_equipa}"}
            CACHE_ESTADIOS[nome_equipa] = result
            return result
    except Exception:
        pass
    time.sleep(1.1)

    # 4. Extrair localidade do nome (ex: "√Åguias de Alvite" ‚Üí "Alvite, Portugal")
    m = re.search(r'\b(?:de|da|do|dos|das)\s+(.+)', nome_equipa, re.IGNORECASE)
    if m:
        localidade = m.group(1).strip()
        try:
            loc = geolocator.geocode(f"{localidade}, Portugal", timeout=5)
            if loc:
                result = {"lat": loc.latitude, "lon": loc.longitude, "local": f"Campo em {localidade}"}
                CACHE_ESTADIOS[nome_equipa] = result
                return result
        except Exception:
            pass
        time.sleep(1.1)

    # 5. Nome da equipa como localidade (ex: "Serzedelo" ‚Üí localidade em PT)
    try:
        loc = geolocator.geocode(f"{nome_equipa}, Portugal", timeout=5)
        if loc:
            result = {"lat": loc.latitude, "lon": loc.longitude, "local": f"Campo {nome_equipa}"}
            CACHE_ESTADIOS[nome_equipa] = result
            return result
    except Exception:
        pass
    time.sleep(1.1)

    # 6. Fallback: centr√≥ide do distrito extra√≠do da competi√ß√£o
    district_geo = _extract_district(comp_text)
    if district_geo:
        print(f"    üìç Fallback distrito para {nome_equipa}: {district_geo['local']}")
        return district_geo

    return None


def is_portuguese_game(casa: str, fora: str, comp_text: str = "",
                       has_pt_flag: bool = False) -> bool:
    """Verifica se o jogo √© portugu√™s (bandeira PT, competi√ß√£o, ou equipas)."""
    if has_pt_flag:
        return True
    cl = comp_text.lower()
    if any(re.search(r'(?:^|\b)' + re.escape(kw) + r'(?:\b|$)', cl)
           for kw in PORTUGUESE_COMP_KEYWORDS):
        return True
    return any(
        _team_match(t, casa) or _team_match(t, fora) for t in PORTUGUESE_TEAMS
    )


def _extract_game_id(url: str) -> str:
    """Extrai o ID num√©rico do jogo a partir do URL para deduplica√ß√£o."""
    m = re.search(r'/(\d{6,})(?:\?|$)', url)
    return m.group(1) if m else url


def parse_games_from_html(html: str) -> list:
    """Extrai jogos do HTML usando BeautifulSoup (li.game + tabela, sempre ambos)."""
    soup = BeautifulSoup(html, "html.parser")
    ids_vistos = set()
    resultados = []

    # --- 1. Tabela principal agenda_list ---
    for row in soup.select("table.agenda_list tr"):
        try:
            time_td = row.select_one("td.time")
            info_td = row.select_one("td.info")
            if not time_td or not info_td:
                continue

            hora = time_td.get_text(strip=True)
            if not re.match(r'\d{2}:\d{2}$', hora):
                continue

            game_link = info_td.select_one(
                "a[href*='/jogo/'], a[href*='/live-ao-minuto/']"
            )
            if not game_link:
                continue
            game_url = game_link.get("href", "")
            gid = _extract_game_id(game_url)
            if gid in ids_vistos:
                continue
            ids_vistos.add(gid)

            game_text = game_link.get_text(strip=True)
            vs_match = re.match(r'(.+?)\s+(?:vs|\d+-\d+)\s+(.+)', game_text)
            if not vs_match:
                # Tentar extrair equipas de outra forma se regex falhar
                continue
            casa = vs_match.group(1).strip()
            fora = vs_match.group(2).strip()

            url_date = re.search(
                r'/(?:jogo|live-ao-minuto)/(\d{4}-\d{2}-\d{2})', game_url
            )
            data = url_date.group(1) if url_date else datetime.now().strftime("%Y-%m-%d")

            comp_el = info_td.select_one("div.match_info")
            comp_text = comp_el.get_text(strip=True) if comp_el else ""

            comp_img = info_td.select_one("div.main_info div.image")
            has_pt_flag = bool(comp_img and "flag:PT" in str(comp_img))

            resultados.append({
                "casa": casa, "fora": fora,
                "data": data, "hora": hora,
                "competicao": comp_text, "url": game_url,
                "has_pt_flag": has_pt_flag,
            })
        except Exception:
            continue

    # --- 2. Elementos li.game (matchbox) ---
    for li in soup.select("li.game"):
        try:
            link = li.select_one(
                "a[href*='/jogo/'], a[href*='/live-ao-minuto/']"
            )
            if not link:
                continue
            game_url = link.get("href", "")
            gid = _extract_game_id(game_url)
            if gid in ids_vistos:
                continue
            ids_vistos.add(gid)

            teams = li.select("div.team span.title")
            if len(teams) < 2:
                continue
            casa = teams[0].get_text(strip=True)
            fora = teams[1].get_text(strip=True)

            url_date = re.search(
                r'/(?:jogo|live-ao-minuto)/(\d{4}-\d{2}-\d{2})', game_url
            )
            data = url_date.group(1) if url_date else None

            hora = None
            date_el = li.select_one("div.date span")
            if date_el:
                tm = re.search(r'(\d{2}:\d{2})', date_el.get_text())
                hora = tm.group(1) if tm else None
            
            if not hora:
                time_tag = li.select_one("span.tag.time")
                if time_tag:
                    t = time_tag.get_text(strip=True)
                    if re.match(r'\d{2}:\d{2}$', t):
                        hora = t

            if not data or not hora:
                continue

            comp_el = li.select_one("div.comp")
            comp_text = re.sub(r'\s+', ' ', comp_el.get_text(strip=True)) if comp_el else ""

            comp_img = li.select_one("div.comp div.image")
            has_pt_flag = bool(comp_img and "flag:PT" in str(comp_img))

            resultados.append({
                "casa": casa, "fora": fora,
                "data": data, "hora": hora,
                "competicao": comp_text, "url": game_url,
                "has_pt_flag": has_pt_flag,
            })
        except Exception:
            continue

    return resultados


async def scrape_game_details(page, game_url: str) -> dict:
    """Visita a p√°gina de um jogo no ZeroZero para extrair URLs de equipas e classifica√ß√£o."""
    result = {"url_equipa_casa": "", "url_equipa_fora": "", "url_classificacao": ""}
    base = "https://www.zerozero.pt"
    
    try:
        full_url = game_url if game_url.startswith("http") else base + game_url
        await page.goto(full_url, timeout=30000, wait_until="domcontentloaded")
        await page.wait_for_timeout(1500)
        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")

        # Extrair URLs das equipas a partir do cabe√ßalho do jogo
        team_urls = []
        header = soup.select_one("#match-header, .match-header, [class*='matchHeader'], [class*='match_header']")
        team_links = (header or soup).select("a[href*='/equipa/']")
        seen_urls = set()
        for tl in team_links:
            href = tl.get("href", "")
            if "/equipa/" in href:
                full = href if href.startswith("http") else base + href
                if full not in seen_urls:
                    seen_urls.add(full)
                    team_urls.append(full)

        if len(team_urls) >= 2:
            result["url_equipa_casa"] = team_urls[0]
            result["url_equipa_fora"] = team_urls[1]
        elif len(team_urls) == 1:
            result["url_equipa_casa"] = team_urls[0]

        # Extrair URL da classifica√ß√£o (link com /edition/ ou classificacao)
        edition_links = soup.select("a[href*='/edition/'], a[href*='classificacao']")
        for el in edition_links:
            href = el.get("href", "")
            if "/edition/" in href or "classificacao" in href:
                result["url_classificacao"] = href if href.startswith("http") else base + href
                break

        # Fallback: procurar link da competi√ß√£o
        if not result["url_classificacao"]:
            comp_links = soup.select("a[href*='/edicao/'], a[href*='/competicao/']")
            for cl in comp_links:
                href = cl.get("href", "")
                if href:
                    result["url_classificacao"] = href if href.startswith("http") else base + href
                    break

    except Exception as e:
        print(f"    ‚ö†Ô∏è Erro ao extrair detalhes de {game_url}: {e}")

    return result


def extrair_escalao(comp_text: str, nome_jogo: str = "") -> str:
    """Extrai o escal√£o do texto da competi√ß√£o ou nome do jogo."""
    texto = (comp_text + " " + nome_jogo).lower()
    if any(x in texto for x in ["sub-19", "juniores a", "juniores"]):
        return "Sub-19"
    if any(x in texto for x in ["sub-17", "juvenis"]):
        return "Sub-17"
    if any(x in texto for x in ["sub-15", "iniciados"]):
        return "Sub-15"
    if any(x in texto for x in ["sub-13", "infantis"]):
        return "Sub-13"
    if any(x in texto for x in ["sub-11", "benjamins", "benjamim"]):
        return "Benjamins"
    if any(x in texto for x in ["sub-9", "sub-7", "traquinas", "petizes"]):
        return "Traquinas"
    if any(x in texto for x in ["revela√ß√£o", "sub-23"]):
        return "Sub-23"
    return "Seniores"


def classificar_evento(comp_text: str, nome_jogo: str = ""):
    """Retorna (categoria, pre√ßo, escal√£o) baseado no texto da competi√ß√£o.

    Pre√ßos m√©dios ponderados do futebol portugu√™s (fontes: Liga Portugal, clubes):
    - Liga Portugal Betclic: ‚Ç¨10-25, m√©dia ~15‚Ç¨
    - Liga Portugal 2: ‚Ç¨5-15, m√©dia ~10‚Ç¨
    - Ta√ßa de Portugal: ‚Ç¨5-15, m√©dia ~8‚Ç¨
    - Liga 3 / Camp. Portugal: ‚Ç¨3-7, m√©dia ~5‚Ç¨
    - Distrital (Seniores): ‚Ç¨2-5, m√©dia ~3‚Ç¨
    - Competi√ß√µes Europeias: ‚Ç¨15-60, m√©dia ~25‚Ç¨
    - Forma√ß√£o (todos os escal√µes): Gr√°tis
    """
    cl = comp_text.lower()
    escalao = extrair_escalao(comp_text, nome_jogo)

    # Forma√ß√£o √© sempre gr√°tis
    if escalao not in ("Seniores", "Sub-23"):
        return f"Forma√ß√£o - {escalao}", "Gr√°tis", escalao

    if any(x in cl for x in ["liga portugal", "primeira liga"]):
        return "Liga Portugal", "~15‚Ç¨ (estimado)", escalao
    if any(x in cl for x in ["liga 3"]):
        return "Liga 3", "~5‚Ç¨ (estimado)", escalao
    if any(x in cl for x in ["liga 2", "segunda liga", "meu super"]):
        return "Liga Portugal 2", "~10‚Ç¨ (estimado)", escalao
    if any(x in cl for x in ["champions", "europa league", "conference"]):
        return "Competi√ß√£o Europeia", "~25‚Ç¨ (estimado)", escalao
    if any(x in cl for x in ["ta√ßa de portugal", "taca de portugal"]):
        return "Ta√ßa de Portugal", "~8‚Ç¨ (estimado)", escalao
    if any(x in cl for x in ["ta√ßa da liga"]):
        return "Ta√ßa da Liga", "~8‚Ç¨ (estimado)", escalao
    if any(x in cl for x in ["revela√ß√£o", "sub-23"]):
        return "Liga Revela√ß√£o", "Gr√°tis", "Sub-23"
    if any(x in cl for x in ["pro-nacional", "campeonato de portugal"]):
        return "Campeonato de Portugal", "~5‚Ç¨ (estimado)", escalao
    if any(x in cl for x in ["divis√£o de honra"]):
        return "Divis√£o de Honra", "~3‚Ç¨ (estimado)", escalao
    return "Futebol Distrital", "~3‚Ç¨ (estimado)", escalao


async def load_page(page, url: str, accept_cookies: bool = False,
                    retries: int = 2) -> str:
    """Carrega uma p√°gina com retry e devolve o HTML."""
    for attempt in range(retries + 1):
        try:
            await page.goto(url, timeout=60000, wait_until="domcontentloaded")

            if accept_cookies:
                try:
                    btn = page.locator("#didomi-notice-agree-button")
                    if await btn.is_visible(timeout=3000):
                        await btn.click()
                        await page.wait_for_timeout(1000)
                except Exception:
                    pass

            try:
                # Esperar por qualquer um dos layouts comuns
                await page.wait_for_selector(
                    "li.game, table.agenda_list", timeout=15000
                )
            except Exception:
                pass

            await page.wait_for_timeout(2000)

            # Scroll para carregar conte√∫do lazy-loaded (divis√µes inferiores)
            for _ in range(10):
                prev_height = await page.evaluate("document.body.scrollHeight")
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(1000)
                new_height = await page.evaluate("document.body.scrollHeight")
                if new_height == prev_height:
                    break

            # Clicar em "ver mais" / "show more" se existir
            try:
                more_btn = page.locator("a.ver_mais, a.show_more, button:has-text('mais'), a:has-text('Ver mais')")
                while await more_btn.first.is_visible(timeout=1000):
                    await more_btn.first.click()
                    await page.wait_for_timeout(1500)
            except Exception:
                pass

            title = await page.title()
            if "cloudflare" in title.lower() or "just a moment" in title.lower():
                raise RuntimeError("Cloudflare challenge detectado")

            return await page.content()

        except Exception as e:
            if attempt < retries:
                wait = 5 * (attempt + 1)
                print(f"  ‚ö†Ô∏è Tentativa {attempt + 1} falhou: {e}. Retry em {wait}s...")
                await asyncio.sleep(wait)
            else:
                print(f"  ‚ùå Falha ap√≥s {retries + 1} tentativas: {e}")
                return ""
    return ""


def limpar_eventos_concluidos():
    """Quinta-feira: limpa eventos passados. Outros dias: mant√©m tudo."""
    hoje = datetime.now()
    if hoje.weekday() != 3:  # 3 = quinta-feira
        print("üìÖ N√£o √© quinta-feira ‚Äî eventos passados mantidos.")
        return

    ontem = (hoje - timedelta(days=1)).strftime("%Y-%m-%d")
    try:
        result = supabase.table("eventos").delete().lt("data", ontem).execute()
        n = len(result.data) if result.data else 0
        print(f"üßπ Quinta-feira: removidos {n} eventos conclu√≠dos.")
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao limpar eventos: {e}")


def extract_games_from_page(html: str, comp_name: str = "") -> list:
    """Extrai jogos de qualquer p√°gina ZeroZero (edi√ß√£o, calend√°rio, competi√ß√£o)."""
    soup = BeautifulSoup(html, "html.parser")
    games = []
    hoje = datetime.now()
    limite = hoje + timedelta(days=7)
    seen = set()

    # Tentar extrair nome da competi√ß√£o do header da p√°gina
    if not comp_name:
        h1 = soup.select_one("h1, .header h2, .comp_title")
        comp_name = h1.get_text(strip=True) if h1 else ""

    for match_link in soup.select("a[href*='/jogo/']"):
        href = match_link.get("href", "")
        if not href or href in seen:
            continue
        seen.add(href)

        # Data do URL do jogo (formato: /jogo/YYYY-MM-DD/...)
        url_date = re.search(r'/jogo/(\d{4}-\d{2}-\d{2})', href)
        if not url_date:
            # Tentar encontrar data no contexto (parent row/div)
            parent = match_link.find_parent(["tr", "li", "div"])
            if parent:
                dm = re.search(r'(\d{4}-\d{2}-\d{2})', str(parent))
                if dm:
                    url_date = dm
        if not url_date:
            continue
        data = url_date.group(1) if hasattr(url_date, 'group') else url_date

        try:
            game_date = datetime.strptime(data, "%Y-%m-%d")
            if game_date.date() < hoje.date() or game_date > limite:
                continue
        except Exception:
            continue

        # Contexto: linha/div que cont√©m o link do jogo
        parent = match_link.find_parent(["tr", "li", "div"])
        if not parent:
            parent = match_link.parent

        # Equipas: links /equipa/ no contexto
        team_links = parent.select("a[href*='/equipa/']") if parent else []
        casa, fora = None, None

        if len(team_links) >= 2:
            casa = team_links[0].get_text(strip=True)
            fora = team_links[1].get_text(strip=True)
        else:
            # Fallback: texto do link do jogo (ex: "Team A vs Team B")
            text = match_link.get_text(strip=True)
            vs = re.match(r'(.+?)\s+(?:vs|x|\d+-\d+)\s+(.+)', text, re.IGNORECASE)
            if vs:
                casa = vs.group(1).strip()
                fora = vs.group(2).strip()

        if not casa or not fora:
            continue

        # Hora
        hora = "A definir"
        ctx_text = parent.get_text() if parent else ""
        time_match = re.search(r'\b(\d{2}:\d{2})\b', ctx_text)
        if time_match:
            hora = time_match.group(1)

        base_url = "https://www.zerozero.pt"
        full_url = href if href.startswith("http") else base_url + href

        games.append({
            "casa": casa, "fora": fora,
            "data": data, "hora": hora,
            "competicao": comp_name, "url": full_url,
            "has_pt_flag": True,
        })

    return games


async def scrape_zerozero():
    base_url = "https://www.zerozero.pt/agenda.php"
    base = "https://www.zerozero.pt"
    print("üåç A iniciar scraping do ZeroZero...")

    # URLs das 20 AFs + competi√ß√µes nacionais (do sitemap zerozero.pt)
    PT_COMPETITION_URLS = {
        "af-algarve": "https://www.zerozero.pt/competicao/af-algarve/169",
        "af-aveiro": "https://www.zerozero.pt/competicao/af-aveiro/170",
        "af-beja": "https://www.zerozero.pt/competicao/af-beja/171",
        "af-braga": "https://www.zerozero.pt/competicao/af-braga/172",
        "af-braganca": "https://www.zerozero.pt/competicao/af-braganca/173",
        "af-castelo-branco": "https://www.zerozero.pt/competicao/af-castelo-branco/174",
        "af-coimbra": "https://www.zerozero.pt/competicao/af-coimbra/175",
        "af-evora": "https://www.zerozero.pt/competicao/af-evora/176",
        "af-guarda": "https://www.zerozero.pt/competicao/af-guarda/177",
        "af-leiria": "https://www.zerozero.pt/competicao/af-leiria/178",
        "af-lisboa": "https://www.zerozero.pt/competicao/af-lisboa/179",
        "af-portalegre": "https://www.zerozero.pt/competicao/af-portalegre/180",
        "af-porto": "https://www.zerozero.pt/competicao/af-porto/181",
        "af-santarem": "https://www.zerozero.pt/competicao/af-santarem/182",
        "af-setubal": "https://www.zerozero.pt/competicao/af-setubal/183",
        "af-viana-do-castelo": "https://www.zerozero.pt/competicao/af-viana-do-castelo/184",
        "af-vila-real": "https://www.zerozero.pt/competicao/af-vila-real/185",
        "af-viseu": "https://www.zerozero.pt/competicao/af-viseu/186",
        "af-ponta-delgada": "https://www.zerozero.pt/competicao/af-ponta-delgada/219",
        "af-madeira": "https://www.zerozero.pt/competicao/af-madeira/222",
        "liga-3": "https://www.zerozero.pt/competicao/iii-divisao/76",
        "juniores-a": "https://www.zerozero.pt/competicao/i-divisao-juniores-a-sub-19-/136",
        "juniores-b": "https://www.zerozero.pt/competicao/i-divisao-juniores-b-sub-17-/137",
        "juniores-c": "https://www.zerozero.pt/competicao/i-divisao-juniores-c-sub-15-/138",
        "feminina": "https://www.zerozero.pt/competicao/liga-portuguesa-feminina/143",
    }

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/121.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1280, "height": 720},
        )
        page = await context.new_page()

        try:
            all_games = []
            ids_vistos = set()
            datas_ok = set()

            # Scrape hoje + pr√≥ximos 6 dias (cobre o fim-de-semana)
            hoje = datetime.now()
            datas = [
                (hoje + timedelta(days=i)).strftime("%Y-%m-%d") for i in range(7)
            ]

            for idx, data_str in enumerate(datas):
                url = f"{base_url}?date={data_str}"
                print(f"üìÖ A processar {data_str}...")

                html = await load_page(page, url, accept_cookies=(idx == 0))
                if not html:
                    continue
                datas_ok.add(data_str)

                jogos = parse_games_from_html(html)
                novos = 0
                for jogo in jogos:
                    gid = _extract_game_id(jogo["url"])
                    if gid not in ids_vistos:
                        ids_vistos.add(gid)
                        all_games.append(jogo)
                        novos += 1
                print(f"   üîç {len(jogos)} jogos na p√°gina, {novos} novos")

            print(f"\nüìä Fase 1 (Agenda): {len(all_games)} jogos encontrados")

            # ================================================================
            # FASE 2: Scraping por AF/competi√ß√£o ‚Äî distritais e forma√ß√£o
            # ================================================================
            print("\nüèüÔ∏è  Fase 2: A descobrir jogos distritais e de forma√ß√£o...")
            af_page = await context.new_page()
            af_total = 0

            for comp_name, comp_url in PT_COMPETITION_URLS.items():
                try:
                    print(f"  üìã {comp_name}...")

                    # 1. Visitar p√°gina da competi√ß√£o/AF
                    html = await load_page(af_page, comp_url)
                    if not html:
                        continue

                    # 2. Descobrir links de edi√ß√µes actuais
                    comp_soup = BeautifulSoup(html, "html.parser")
                    edition_urls = []
                    for link in comp_soup.select("a[href*='/edicao/']"):
                        href = link.get("href", "")
                        if href:
                            full = href if href.startswith("http") else base + href
                            if full not in edition_urls:
                                edition_urls.append(full)

                    # Limitar a 20 edi√ß√µes por AF (evitar runaway)
                    edition_urls = edition_urls[:20]

                    if not edition_urls:
                        # Sem edi√ß√µes ‚Äî tentar extrair jogos directamente da p√°gina
                        jogos = extract_games_from_page(html, comp_name)
                        for j in jogos:
                            gid = _extract_game_id(j["url"])
                            if gid not in ids_vistos:
                                ids_vistos.add(gid)
                                all_games.append(j)
                                af_total += 1
                        continue

                    # 3. Para cada edi√ß√£o, visitar calend√°rio e extrair jogos
                    for ed_url in edition_urls:
                        try:
                            # Tentar URL do calend√°rio directamente
                            cal_url = ed_url.rstrip("/") + "/calendario"
                            html_cal = await load_page(af_page, cal_url)
                            if not html_cal:
                                html_cal = await load_page(af_page, ed_url)
                            if not html_cal:
                                continue

                            # Extrair nome da competi√ß√£o da edi√ß√£o
                            ed_soup = BeautifulSoup(html_cal, "html.parser")
                            ed_h1 = ed_soup.select_one("h1, h2.header_title")
                            ed_comp = ed_h1.get_text(strip=True) if ed_h1 else comp_name

                            jogos = extract_games_from_page(html_cal, ed_comp)

                            novos_ed = 0
                            for j in jogos:
                                gid = _extract_game_id(j["url"])
                                if gid not in ids_vistos:
                                    ids_vistos.add(gid)
                                    all_games.append(j)
                                    novos_ed += 1
                                    af_total += 1

                            if novos_ed:
                                print(f"     ‚úÖ {ed_comp}: +{novos_ed} jogos")

                            await asyncio.sleep(0.5)
                        except Exception as e:
                            print(f"     ‚ö†Ô∏è Erro edi√ß√£o {ed_url}: {e}")
                            continue

                    await asyncio.sleep(0.3)
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Erro {comp_name}: {e}")
                    continue

            await af_page.close()
            print(f"\nüìä Fase 2 (AFs): +{af_total} jogos distritais/forma√ß√£o")
            print(f"üìä Total: {len(all_games)} jogos encontrados")

            # Filtrar: manter apenas jogos portugueses relevantes
            jogos_pt = []
            skipped_geo = 0
            for jogo in all_games:
                casa, fora = jogo["casa"], jogo["fora"]
                comp = jogo["competicao"]
                
                if not is_portuguese_game(
                    casa, fora, comp, jogo.get("has_pt_flag", False)
                ):
                    continue

                geo = (
                    geolocalizar_estadio(casa, comp)
                    or geolocalizar_estadio(fora, comp)
                )
                if not geo:
                    skipped_geo += 1
                    print(f"    ‚ö†Ô∏è Sem geolocaliza√ß√£o: {casa} vs {fora} ({comp})")
                    continue

                jogo["_geo"] = geo
                jogos_pt.append(jogo)

            print(f"\n‚öΩ {len(jogos_pt)} jogos portugueses ({skipped_geo} sem geolocaliza√ß√£o)")

            # Visitar p√°ginas de detalhe dos jogos para extrair URLs
            print(f"üîó A extrair detalhes de {len(jogos_pt)} jogos...")
            detail_page = await context.new_page()
            
            resultados = []
            for i, jogo in enumerate(jogos_pt):
                casa, fora = jogo["casa"], jogo["fora"]
                geo = jogo["_geo"]
                
                # Extrair detalhes da p√°gina do jogo (URLs das equipas + classifica√ß√£o)
                details = {"url_equipa_casa": "", "url_equipa_fora": "", "url_classificacao": ""}
                if jogo.get("url"):
                    try:
                        details = await scrape_game_details(detail_page, jogo["url"])
                        if (i + 1) % 10 == 0:
                            print(f"   üìÑ {i + 1}/{len(jogos_pt)} detalhes extra√≠dos...")
                        await asyncio.sleep(0.5)  # Rate limiting
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è Saltar detalhes de {casa} vs {fora}: {e}")

                cat, preco, escalao = classificar_evento(jogo["competicao"], f"{casa} vs {fora}")

                evento = {
                    "nome": f"{casa} vs {fora}",
                    "tipo": "Futebol",
                    "categoria": cat,
                    "escalao": escalao,
                    "equipa_casa": casa,
                    "equipa_fora": fora,
                    "url_jogo": jogo.get("url", ""),
                    "url_equipa_casa": details.get("url_equipa_casa", ""),
                    "url_equipa_fora": details.get("url_equipa_fora", ""),
                    "url_classificacao": details.get("url_classificacao", ""),
                    "data": jogo["data"],
                    "hora": jogo["hora"],
                    "local": geo["local"],
                    "latitude": geo["lat"],
                    "longitude": geo["lon"],
                    "preco": preco,
                    "descricao": f"Jogo de {cat}. {jogo['competicao']}",
                    "url_maps": (
                        f"https://www.google.com/maps/search/"
                        f"?api=1&query={geo['lat']},{geo['lon']}"
                    ),
                    "status": "aprovado",
                }
                resultados.append(evento)
                print(f"  ‚úÖ {evento['nome']} ({jogo['data']} {jogo['hora']})")

            await detail_page.close()
            return resultados, datas_ok

        except Exception as e:
            print(f"‚ùå Erro Scraping: {e}")
            return [], set()
        finally:
            await browser.close()


def verificar_adiamentos(eventos_novos: list, datas_ok: set):
    """Compara eventos futuros na DB com scrape fresco para detectar adiamentos."""
    hoje = datetime.now().strftime("%Y-%m-%d")

    try:
        result = (
            supabase.table("eventos")
            .select("*")
            .gte("data", hoje)
            .eq("status", "aprovado")
            .eq("tipo", "Futebol")
            .execute()
        )
        eventos_db = result.data or []
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao verificar adiamentos: {e}")
        return

    # Jogos encontrados no scrape: (nome, data)
    scrape_set = {(ev["nome"], ev["data"]) for ev in eventos_novos}

    # Nomes ‚Üí datas no scrape (para detectar remarca√ß√µes)
    nomes_datas = {}
    for ev in eventos_novos:
        nomes_datas.setdefault(ev["nome"], []).append(ev["data"])

    adiados = 0
    remarcados = 0

    for ev_db in eventos_db:
        nome, data = ev_db["nome"], ev_db["data"]

        # S√≥ verificar datas que foram scrapeadas com sucesso
        if data not in datas_ok:
            continue

        if (nome, data) in scrape_set:
            continue

        # Jogo n√£o aparece no scrape para esta data
        if nome in nomes_datas:
            novas = [d for d in nomes_datas[nome] if d != data]
            if novas:
                supabase.table("eventos").update({
                    "status": "adiado",
                    "descricao": f"‚ö†Ô∏è Remarcado para {novas[0]}. " + (ev_db.get("descricao") or ""),
                }).eq("id", ev_db["id"]).execute()
                remarcados += 1
                print(f"  üîÑ Remarcado: {nome} ({data} ‚Üí {novas[0]})")
                continue

        supabase.table("eventos").update({
            "status": "adiado",
        }).eq("id", ev_db["id"]).execute()
        adiados += 1
        print(f"  ‚ö†Ô∏è Adiado: {nome} ({data})")

    if adiados or remarcados:
        print(f"üìã Adiamentos: {adiados} adiados, {remarcados} remarcados")
    else:
        print("üìã Sem adiamentos detectados")


async def main():
    # 1. Quinta-feira: limpar eventos conclu√≠dos
    limpar_eventos_concluidos()

    # 2. Scrape de novos eventos
    eventos, datas_ok = await scrape_zerozero()

    if not eventos:
        print("‚ö†Ô∏è Nenhum evento portugu√™s encontrado.")
        return

    # 3. Verificar adiamentos (antes de guardar novos)
    print("\nüîç A verificar adiamentos...")
    verificar_adiamentos(eventos, datas_ok)

    # 4. Guardar na base de dados (upsert: atualiza existentes, insere novos)
    print(f"\nüì¶ A guardar {len(eventos)} eventos no Supabase...")
    guardados = 0
    erros = 0
    for ev in eventos:
        try:
            supabase.table("eventos").upsert(ev, on_conflict="nome,data").execute()
            guardados += 1
        except Exception as e:
            erros += 1
            if erros <= 5:
                print(f"  Erro DB: {e}")
            elif erros == 6:
                print("  ... (mais erros omitidos)")

    print(f"üèÅ Feito. {guardados}/{len(eventos)} eventos guardados ({erros} erros).")


if __name__ == "__main__":
    asyncio.run(main())
